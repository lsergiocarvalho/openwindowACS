# openwindowACS
Opening the windows to improve risk prediction among individuals with premature acute coronary syndromes


Model Development Process

To develop the prognostic models, B-CaRe:QCO data were extracted into a labelled dataset containing the independent variables (using the patients’ clinical records at their baseline dates or during index hospitalization) and all dependent variable (occurrence of a composite endpoint of death due to cardiovascular causes and recurrent ACS following the baseline date). 

We implemented a grid search for the hyper-parameter optimisation using the method reported by Bergstra and Bengio. This requires the operator to specify a range of values for each hyper-parameters and all possible combinations of the hyper-parameters are investigated, with the combination corresponding to the highest cross-validation performance metric (in this case, maximization of the C-statistics being chosen for the final model). The justification for selecting the hyper-parameters that maximises the C-statistics is that this is less affected when the labelled data are unbalanced compared to using accuracy as metric. When the classes are unbalanced, it is also a common strategy to over-sample the rare label data and under-sample the common label data as many machine learning models can be sensitive to unbalanced data. Below, we describe in further detail the algorithms used.


Short-term predictive algorithms for classification

Random Forests. For the hyper-parameter grid search we investigated ntree = 50, 150, and 350; mtry from 5 up to the maximum number of variables in increments of 5; max depth = 2, 4, 6, 8, and 10; row sample of 90%, 95% and 100%. The chosen (optimal) random forest model had the hyper-parameters: ntree = 350, mtry = 25, max depth = 5 (up to 5 variable interactions were used by the model) and row sample fraction of 0.95 (95% of the data points were used to train each tree).

XGboost. The grid search for the hyper-parameters investigated in our models were: ntree = 25, 50, 75 and 100; max depth = 2, 3, 4, 6 and 8; the minimum observations per node was 5, 10, 20, and 40. The gradient boosting machine model was chosen to have a Bernoulli distribution and the chosen model had the hyper-parameters: ntree = 50, max depth = 3 (up to 3 variable interactions were used by the model) and the minimum number of observations per node was 10. XGBoost was implemented in Python.

TabNet. We used a canonical deep neural network (DNN) architecture for tabular data described by Arik et al. Briefly, TabNet is trained using gradient descent-based optimization and uses sequential attention to choose which features to reason from at each decision step, enabling (i) interpretability, (ii) more accurate and faster learning and (iii) flexible integration into end-to-end learning. Through sparse and instance-wise selection (sparsemax is used for normalization of the coefficients) of features with highest impact on outcomes, the learning capacity of a decision step is not wasted on irrelevant ones, and thus the model becomes more parameter efficient. TabNet also constructs a sequential multi-step architecture, where each step contributes to a portion of the decision based on the selected features; improves the learning capacity via nonlinear processing of the selected features; and mimics ensembling via higher dimensions. TabNet encoder is composed of a feature transformer, an attentive transformer and feature masking. A split block divides the processed representation to be used by the attentive transformer of the subsequent step as well as for the overall output. For each step, the feature selection mask provides interpretable information about the model’s functionality, and the masks can be aggregated to obtain global feature important attribution. TabNet decoder is composed of a feature transformer block at each step. Each feature transformer block is composed of a 4-layer network, where 2 are shared across all decision steps and 2 are decision step-dependent. Each layer is composed of a fully-connected (FC) layer, ghost batch normalization (BN) and gated linear unit (GLU) nonlinearity. We used standard classification (softmax cross entropy) loss functions and we train model until convergence using unsupervised pre-training. The final TabNet model was implemented in pytorch environment and had the following configuration: Adam optimizer with learning rate of 0.02 and a decay rate of 0.9 every 10 interactions, Glorot uniform initialization, batch size of 256, Max epoch 1000, workers at zero, momentum of 0.9, Nsteps=8, γ=2.0, weight at 1 (automated sampling). 

Logistic regression models. We built series of stepwise logistic regression models to predict in-hospital MACE.


Long-term predictive models – survival with competing risks

Cause-specific Cox-proportional hazards model (Cox) and Fine-Gray proportional sub-distribution hazards model (Fine-Gray). The Cox model relates the covariates to the hazard function of the outcome of interest and not directly to the survival times themselves. The covariates have a relative effect on the hazard function because of the use of the logarithmic transformation and the regression coefficients are interpreted as log-hazard ratios. The hazard ratio is equal to the exponential of the associated regression coefficient9. Competing risks implies that a subject can experience one of a set of different events or outcomes. In this case, two different types of hazard functions are of interest: the cause-specific hazard function and the subdistribution hazard function. The cause-specific hazard function indicates the instantaneous rate of occurrence of the kth event in subjects who are currently event free (ie, in subjects who have not yet experienced any of the different types of events). Considering two types of events, death attributable to cardiovascular causes and death attributable to non-cardiovascular causes, then the cause-specific hazard of cardiovascular death denotes the instantaneous rate of cardiovascular death in subjects who are still alive. It denotes the instantaneous risk of failure from the kth event in subjects who have not yet experienced an event of type k. There is a distinct cause-specific hazard function for each of the distinct types of events and a distinct subdistribution hazard function for each of the distinct types of events. In settings in which competing risks are present, two different hazard regression models are available: modeling the cause-specific hazard and modeling the subdistribution hazard function. The second model has also been described as a cumulative incidence function (CIF) regression model, what means that the subdistribution hazard model allows one to estimate the effect of covariates on the cumulative incidence function for the event of interest. However, it is recommended to use the Fine-Gray (FG) subdistribution hazard model when the focus is on estimating incidence or predicting prognosis in the presence of competing risks, since this model generally show better accuracy compared to Cox model. The (cause-specific) cumulative incidence function (CIF) expresses the probability that a particular event k* occurs on or before time t∗ conditional on covariates x*. Since true CIF is not known, the model utilizes estimated CIF to compare the risk of event occurring and to assess how models discriminate across cause-specific risks among patients. Model performance was calculated by using the time-dependent concordance index Ctd (Ctd-index), which measures the extent to which the ordering of actual survival times of pairs agrees with the ordering of their predicted risk. Cox and FG benchmarks were run using the R libraries survival and cmprsk. We estimate the time-dependent Ctd-index for the survival analysis methods under consideration using the function cindex of the R-package pec.

Deep multi-task Gaussian process (DMGP) is a nonparametric Bayesian model for survival analysis that relies on a conception of the competing risks problem as a multi-task learning problem; i.e., it models the cause-specific survival times as the outputs of a random vector-valued function, the inputs to which are the patients’ covariates. This allows the model to learn a “shared representation” of survival times with respect to multiple related comorbidities. Inference of patient-specific posterior survival distribution is conducted via a variational Bayes algorithm. By using inducing variables to derive a variational lower bound on the marginal likelihood of the observed time-to-event data, which is maximized using the adaptive moment estimation algorithm (Adam). Hyperparameters ΘZ and ΘT were tuned using the offline B-CaRe:QCO dataset; and for any out-of-sample patient with all covariates, DMGP evaluates posterior probability density by direct Monte Carlo sampling. Hyperparameters were calibrated by maximizing the marginal likelihood of posterior probability density. DMGP was implemented in Python.

DeepHit trains a neural network to learn the estimated joint distribution of of survival time and event, while capturing the right-censored nature inherent in survival data. The network is trained by using a loss function that exploits both survival times and relative risks. DeepHit makes no assumptions about the underlying stochastic process and allows for the possibility that the relationship between covariates and risks changes over time. DeepHit is a multi-task network which consists of a shared sub-network and K cause-specific sub-networks, differing from that of conventional multi-task network in two ways: (i) it utilizes a single softmax layer as the output layer of DeepHit in order to ensure that the network learns the joint distribution of K competing events not the marginal distributions of each event; (ii) it keeps a residual connection from the input covariates into the input of each cause-specific sub-network. To train DeepHit, a total loss function LTotal is specifically designed to handle censored data. This loss function is the sum of two terms LTotal = L1 + L2; L1 is the log-likelihood of the joint distribution of the first hitting time and event; L2 incorporates a combination of cause-specific ranking loss functions which adapts the idea of concordance. The hyper-parameters for LTotal were selected based on the discriminative performance on the validation set. Early stopping was performed based on the total loss. DeepHit is a 4-layer network consisting of 1 fully-connected layer for the shared sub-network and 2 fully-connected layers for each cause-specific sub-network and a softmax layer as the output layer. For hidden layers, the number of nodes were set as 3, 5, and 3 times of the covariate dimension for the layer 1, 2, and 3, respectively, with ReLu activation function. The network was trained by back-propagation via Adam optimizer with a batch size of 50 and a learning rate of 0.0001. Dropout probability of 0.6 and Xavier initialization was applied for all the layers. DeepHit was implemented in a Tensorflow environment in Python.


References

1.	Ryan H, Trosclair A, Gfroerer J. Adult current smoking: differences in definitions and prevalence estimates--NHIS and NSDUH, 2008. J Environ Public Health. 2012;2012:918368.
2.	Fox KA, Dabbous OH, Goldberg RJ, et al. Prediction of risk of death and myocardial infarction in the six months after presentation with acute coronary syndrome: prospective multinational observational study (GRACE). BMJ. 2006;333(7578):1091.
3.	Thygesen K, Alpert JS, Jaffe AS, et al. Fourth Universal Definition of Myocardial Infarction (2018). Circulation. 2018;138(20):e618-e651.
4.	Heinze G, Wallisch C, Dunkler D. Variable selection - A review and recommendations for the practicing statistician. Biom J. 2018;60(3):431-449.
5.	Belsley; DA, Kuh; E, Welsch RE. Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. 2nd edition ed: Wiley Interscience; 2013.
6.	Granger CB, Goldberg RJ, Dabbous O, et al. Predictors of hospital mortality in the global registry of acute coronary events. Arch Intern Med. 2003;163(19):2345-2353.
7.	Bergstra J, Bengio Y. Random search for hyper-parameter optimization. Journal of Machine Learning Research. 2012;13:281–305.
8.	Arik SO, Pfister T. TabNet: Attentive Interpretable Tabular Learning. Association for the Advancement of Artificial Intelligence; 2020.
9.	Austin PC, Lee DS, Fine JP. Introduction to the Analysis of Survival Data in the Presence of Competing Risks. Circulation. 2016;133(6):601-609.
10.	Antolini L, Boracchi P, Biganzoli E. A time-dependent discrimination index for survival data. . Statistics in Medicine 2005;24:3927–3944.
11.	Alaa AM, van der Schaar M. Deep multi-task gaussian processes for survival analysis with competing risks. 30th Conference on Neural Information Processing Systems; 2017.
12.	Lee C, Zame WR, Yoon J, Schaar Mvd. DeepHit: A Deep Learning Approach to Survival Analysis With Competing Risks. Paper presented at: XXXII Association for the Advancement of Artificial Intelligence (AAAI) Conference2018.
